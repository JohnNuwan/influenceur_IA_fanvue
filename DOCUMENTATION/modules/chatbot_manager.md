# ğŸ¤– Module de Gestion des Chatbots

## ğŸ“‹ Vue d'ensemble

Ce module gÃ¨re les chatbots automatiques pour interagir avec les abonnÃ©s sur les plateformes de contenu (Fanvue, OnlyFans, Telegram, Discord) et automatiser les ventes de packs.

**Approche Open Source :** Ce module utilise **Ollama** comme LLM local pour les rÃ©ponses intelligentes, offrant un contrÃ´le total et une rÃ©duction des coÃ»ts.

---

## ğŸ¯ FonctionnalitÃ©s principales

### 1. RÃ©ponses automatiques avec Ollama
- **Messages d'accueil** : Accueil automatique des nouveaux abonnÃ©s
- **RÃ©ponses contextuelles** : RÃ©ponses intelligentes avec LLM local
- **Vente automatique** : Proposition de packs selon les demandes
- **Support client** : RÃ©ponses aux questions frÃ©quentes

### 2. Gestion des conversations
- **Suivi des conversations** : Historique complet des Ã©changes
- **Classification des messages** : CatÃ©gorisation automatique
- **Escalade** : Transfert vers support humain si nÃ©cessaire
- **Personnalisation** : Adaptation selon le profil utilisateur

### 3. IntÃ©gration plateformes
- **Fanvue** : IntÃ©gration API officielle
- **OnlyFans** : IntÃ©gration via API ou web scraping
- **Telegram** : Bot Telegram personnalisÃ©
- **Discord** : Bot Discord pour serveurs privÃ©s

---

## ğŸ—ï¸ Architecture technique

### Architecture Ollama
```mermaid
graph TB
    subgraph "Platforms"
        A[Fanvue API]
        B[OnlyFans API]
        C[Telegram Bot]
        D[Discord Bot]
    end
    
    subgraph "Chatbot Manager"
        E[Message Receiver]
        F[Intent Classifier]
        G[Context Manager]
        H[Response Generator]
        I[Action Executor]
    end
    
    subgraph "Ollama LLM"
        J[Ollama Service<br/>llama2:7b]
        K[Ollama Service<br/>mistral:7b]
        L[Prompt Manager]
        M[Response Validator]
    end
    
    subgraph "Storage"
        N[Conversation DB]
        O[User Profiles]
        P[Vector DB<br/>Chroma]
    end
    
    A --> E
    B --> E
    C --> E
    D --> E
    
    E --> F
    F --> G
    G --> H
    H --> I
    
    H --> J
    H --> K
    H --> L
    L --> M
    
    G --> N
    G --> O
    G --> P
    
    I --> A
    I --> B
    I --> C
    I --> D
```

### Flux de conversation avec Ollama
```mermaid
sequenceDiagram
    participant U as User
    participant P as Platform
    participant CM as Chatbot Manager
    participant O as Ollama
    participant DB as Database
    
    U->>P: Envoie message
    P->>CM: Webhook/API call
    CM->>CM: Analyse du message
    CM->>DB: RÃ©cupÃ¨re contexte utilisateur
    CM->>O: GÃ©nÃ¨re rÃ©ponse avec prompt
    O->>CM: Retourne rÃ©ponse
    CM->>CM: Post-processing
    CM->>DB: Sauvegarde conversation
    CM->>P: Envoie rÃ©ponse
    P->>U: Affiche rÃ©ponse
```

### Structure des donnÃ©es
```json
{
  "conversation_id": "uuid",
  "user_id": "user_123",
  "platform": "fanvue",
  "status": "active",
  "messages": [
    {
      "message_id": "msg_001",
      "timestamp": "2024-01-15T10:30:00Z",
      "sender": "user",
      "content": "Salut, tu as des photos de pieds ?",
      "type": "text",
      "intent": "feet_request"
    },
    {
      "message_id": "msg_002",
      "timestamp": "2024-01-15T10:30:30Z",
      "sender": "bot",
      "content": "Salut beautÃ© ğŸ˜˜ Oui j'ai un pack pieds exclusif !",
      "type": "text",
      "intent": "feet_response",
      "generated_by": "ollama"
    }
  ],
  "user_profile": {
    "subscription_level": "premium",
    "purchase_history": ["pack_001", "pack_002"],
    "preferences": ["feet", "lingerie"],
    "last_activity": "2024-01-15T10:30:00Z"
  },
  "bot_state": {
    "current_intent": "feet_sales",
    "context": {
      "offered_pack": "feet_pack_001",
      "price": 10.00
    },
    "ollama_model": "llama2:7b",
    "conversation_history": "string"
  }
}
```

### Flux de conversation avec Ollama
```
1. RÃ©ception du message
   â”œâ”€â”€ Analyse du contenu
   â”œâ”€â”€ Classification de l'intention
   â””â”€â”€ Mise Ã  jour du contexte

2. GÃ©nÃ©ration de la rÃ©ponse (Ollama)
   â”œâ”€â”€ PrÃ©paration du prompt
   â”œâ”€â”€ Appel Ã  Ollama API
   â”œâ”€â”€ Post-processing de la rÃ©ponse
   â””â”€â”€ VÃ©rification des rÃ¨gles

3. Envoi de la rÃ©ponse
   â”œâ”€â”€ Formatage du message
   â”œâ”€â”€ Envoi via API
   â””â”€â”€ Log de l'action

4. Suivi post-rÃ©ponse
   â”œâ”€â”€ Mise Ã  jour des mÃ©triques
   â”œâ”€â”€ Gestion des actions
   â””â”€â”€ Planification des follow-ups
```

---

## ğŸ”§ Configuration Ollama

### Installation et Configuration
```yaml
# docker-compose.yml
services:
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    command: >
      sh -c "
        ollama pull llama2:7b &&
        ollama pull mistral:7b &&
        ollama serve
      "
```

### ModÃ¨les Ollama recommandÃ©s
```yaml
models:
  primary:
    name: "llama2:7b"
    description: "ModÃ¨le principal pour les rÃ©ponses gÃ©nÃ©rales"
    parameters:
      temperature: 0.7
      top_p: 0.9
      max_tokens: 500
  
  sales:
    name: "mistral:7b"
    description: "ModÃ¨le spÃ©cialisÃ© pour les ventes"
    parameters:
      temperature: 0.8
      top_p: 0.95
      max_tokens: 300
  
  support:
    name: "llama2:7b"
    description: "ModÃ¨le pour le support client"
    parameters:
      temperature: 0.5
      top_p: 0.8
      max_tokens: 400
```

### Configuration des prompts
```yaml
prompts:
  system_prompt: |
    Tu es une influenceuse NSFW franÃ§aise qui gÃ¨re des conversations avec ses abonnÃ©s.
    Tu dois Ãªtre chaleureuse, sÃ©ductrice mais professionnelle.
    Tu vends des packs de photos (lingerie, pieds, nude artistique).
    RÃ©ponds toujours en franÃ§ais avec des emojis appropriÃ©s.
    Ne sois jamais vulgaire ou explicite, reste dans le cadre du glamour.

  context_template: |
    Contexte utilisateur:
    - Niveau d'abonnement: {subscription_level}
    - Historique d'achat: {purchase_history}
    - PrÃ©fÃ©rences: {preferences}
    - DerniÃ¨re activitÃ©: {last_activity}
    
    Conversation rÃ©cente:
    {conversation_history}
    
    Message actuel: {current_message}
    
    GÃ©nÃ¨re une rÃ©ponse appropriÃ©e et engageante.

  sales_prompt: |
    L'utilisateur semble intÃ©ressÃ© par {pack_type}.
    Propose-lui le pack {pack_id} au prix de {price}â‚¬.
    Sois persuasive mais pas insistante.
    Inclus des dÃ©tails sur le contenu du pack.
```

---

## ğŸ”§ Configuration des plateformes

### Fanvue API
```yaml
api_key: "your_fanvue_api_key"
api_secret: "your_fanvue_api_secret"
webhook_url: "https://your-domain.com/webhook/fanvue"

features:
  - auto_response
  - message_history
  - user_management
  - sales_tracking

limits:
  messages_per_minute: 10
  daily_messages: 1000
```

### OnlyFans API
```yaml
api_key: "your_onlyfans_api_key"
api_secret: "your_onlyfans_api_secret"
webhook_url: "https://your-domain.com/webhook/onlyfans"

features:
  - auto_response
  - content_management
  - subscription_management
  - analytics

limits:
  messages_per_minute: 5
  daily_messages: 500
```

### Telegram Bot
```yaml
bot_token: "your_telegram_bot_token"
webhook_url: "https://your-domain.com/webhook/telegram"

commands:
  - /start: "Accueil et prÃ©sentation"
  - /packs: "Liste des packs disponibles"
  - /pricing: "Tarifs et promotions"
  - /help: "Aide et support"

features:
  - inline_keyboards
  - media_support
  - group_chat_support
```

### Discord Bot
```yaml
bot_token: "your_discord_bot_token"
guild_id: "your_guild_id"
webhook_url: "https://your-domain.com/webhook/discord"

channels:
  - name: "general"
    permissions: ["read", "send_messages"]
  - name: "nsfw"
    permissions: ["read", "send_messages", "attach_files"]

features:
  - slash_commands
  - role_management
  - moderation_tools
```

---

## ğŸ“ Templates de rÃ©ponses avec Ollama

### Messages d'accueil
```yaml
templates:
  welcome_new_subscriber:
    fanvue:
      prompt: |
        L'utilisateur vient de s'abonner. Accueille-le chaleureusement et propose-lui de dÃ©couvrir tes packs.
        Sois sÃ©ductrice mais pas vulgaire.
      ollama_model: "llama2:7b"
      delay: 30
      follow_up: "pieds_offer"
    
    onlyfans:
      prompt: |
        Nouvel abonnÃ© sur OnlyFans. Accueille-le et prÃ©sente tes services.
        Mentionne tes packs exclusifs.
      ollama_model: "llama2:7b"
      delay: 45
      follow_up: "lingerie_offer"
```

### RÃ©ponses aux demandes
```yaml
templates:
  feet_request:
    prompt: |
      L'utilisateur demande des photos de pieds.
      Propose-lui le pack pieds exclusif Ã  10â‚¬.
      Sois sÃ©ductrice et persuasive.
    ollama_model: "mistral:7b"
    actions:
      - type: "offer_pack"
        pack_id: "feet_pack_001"
        price: 10.00
      - type: "schedule_follow_up"
        delay: 3600
        message: "Tu as pensÃ© Ã  mon pack pieds ? ğŸ˜ Il est toujours disponible !"
    
  lingerie_request:
    prompt: |
      L'utilisateur s'intÃ©resse aux photos lingerie.
      Propose le pack lingerie Ã  15â‚¬.
      DÃ©cris le contenu de maniÃ¨re attrayante.
    ollama_model: "mistral:7b"
    actions:
      - type: "offer_pack"
        pack_id: "lingerie_pack_001"
        price: 15.00
      - type: "schedule_follow_up"
        delay: 7200
        message: "Mon pack lingerie t'attend toujours... ğŸ”¥"
    
  nude_request:
    prompt: |
      L'utilisateur demande des photos nues.
      Propose le pack nude artistique Ã  25â‚¬.
      Reste dans le cadre du glamour.
    ollama_model: "mistral:7b"
    actions:
      - type: "offer_pack"
        pack_id: "nude_pack_001"
        price: 25.00
      - type: "schedule_follow_up"
        delay: 10800
        message: "Mon pack nude est toujours lÃ  si tu veux... ğŸ’‹"
```

### RÃ©ponses de support
```yaml
templates:
  pricing_question:
    prompt: |
      L'utilisateur demande les tarifs.
      Liste tous tes packs avec leurs prix.
      Sois claire et professionnelle.
    ollama_model: "llama2:7b"
    
  technical_issue:
    prompt: |
      L'utilisateur a un problÃ¨me technique.
      Sois empathique et propose de l'aider.
      Demande plus de dÃ©tails.
    ollama_model: "llama2:7b"
    escalation: true
    
  spam_detection:
    prompt: |
      L'utilisateur envoie trop de messages.
      Sois polie mais ferme.
      Demande-lui d'Ãªtre plus spÃ©cifique.
    ollama_model: "llama2:7b"
    cooldown: 300
```

---

## ğŸ”„ Workflow automatisÃ© avec Ollama

### Processus de rÃ©ponse
```mermaid
flowchart TD
    A[Message reÃ§u] --> B{Analyse du contenu}
    B --> C[Classification intention]
    C --> D[RÃ©cupÃ©ration contexte]
    D --> E[PrÃ©paration prompt]
    E --> F[Appel Ollama API]
    F --> G{Validation rÃ©ponse}
    G -->|OK| H[Post-processing]
    G -->|KO| I[GÃ©nÃ©ration fallback]
    H --> J[Envoi rÃ©ponse]
    I --> J
    J --> K[Sauvegarde conversation]
    K --> L[Mise Ã  jour mÃ©triques]
```

### Gestion des ventes avec IA
```
1. DÃ©tection de l'intÃ©rÃªt
   â”œâ”€â”€ Analyse des mots-clÃ©s
   â”œâ”€â”€ Historique d'achat
   â””â”€â”€ Comportement utilisateur

2. Proposition de pack (Ollama)
   â”œâ”€â”€ GÃ©nÃ©ration de proposition personnalisÃ©e
   â”œâ”€â”€ SÃ©lection du pack appropriÃ©
   â””â”€â”€ Envoi du lien d'achat

3. Suivi de la vente
   â”œâ”€â”€ Confirmation d'achat
   â”œâ”€â”€ Envoi du contenu
   â””â”€â”€ Demande de feedback

4. Follow-up intelligent
   â”œâ”€â”€ Remerciement personnalisÃ©
   â”œâ”€â”€ Proposition d'autres packs
   â””â”€â”€ Demande de recommandation
```

---

## ğŸ“Š MÃ©triques et KPIs

### MÃ©triques de performance
- **Taux de rÃ©ponse** : Pourcentage de messages rÃ©pondus
- **Temps de rÃ©ponse** : Temps moyen de rÃ©ponse (objectif < 20s)
- **Taux de conversion** : Pourcentage de ventes rÃ©ussies
- **Satisfaction client** : Score de satisfaction
- **Performance Ollama** : Temps de gÃ©nÃ©ration, qualitÃ© des rÃ©ponses

### Alertes automatiques
- **Temps de rÃ©ponse Ã©levÃ©** : Plus de 30 secondes
- **Taux de conversion faible** : Moins de 5%
- **Spam dÃ©tectÃ©** : Trop de messages d'un utilisateur
- **Erreur Ollama** : ProblÃ¨me avec le LLM local
- **Erreur technique** : ProblÃ¨me avec l'API

---

## ğŸ› ï¸ IntÃ©gration avec d'autres modules

### Module de gestion des ventes
- **Proposition de packs** : IntÃ©gration avec le catalogue
- **Suivi des transactions** : Confirmation des achats
- **Gestion des promotions** : Codes promo automatiques

### Module d'analytics
- **MÃ©triques de conversation** : Envoi des statistiques
- **Analyse des comportements** : Patterns d'utilisation
- **Optimisation** : Suggestions d'amÃ©lioration

### Module de gÃ©nÃ©ration de contenu
- **Demande de contenu** : GÃ©nÃ©ration selon les demandes
- **Personnalisation** : Contenu adaptÃ© aux prÃ©fÃ©rences
- **Feedback** : AmÃ©lioration basÃ©e sur les retours

---

## ğŸ” SÃ©curitÃ© et conformitÃ©

### SÃ©curitÃ©
- **Chiffrement** : Chiffrement des conversations
- **Authentification** : VÃ©rification des utilisateurs
- **Audit** : Logs de toutes les interactions
- **Rate limiting** : Limitation des messages
- **Validation Ollama** : Filtrage des rÃ©ponses inappropriÃ©es

### ConformitÃ©
- **RGPD** : Gestion des donnÃ©es personnelles
- **Age verification** : VÃ©rification de l'Ã¢ge
- **Content guidelines** : Respect des guidelines
- **Data retention** : Politique de conservation

---

## ğŸš€ Plan d'implÃ©mentation

### Phase 1 : MVP avec Ollama (3 semaines)
1. **Semaine 1** : Setup Ollama et intÃ©gration de base
2. **Semaine 2** : Templates de base et rÃ©ponses automatiques
3. **Semaine 3** : Gestion des ventes et follow-ups

### Phase 2 : Extension (2 semaines)
1. **Semaine 4** : IntÃ©gration OnlyFans et Telegram
2. **Semaine 5** : Analytics et optimisation

### Phase 3 : Optimisation (1 semaine)
1. **Semaine 6** : A/B testing et amÃ©lioration des performances

---

## ğŸ“ Exemples d'utilisation

### Configuration du chatbot avec Ollama
```python
# Exemple d'utilisation du module
from chatbot_manager import ChatbotManager

manager = ChatbotManager()

# Configuration pour Ollama
ollama_config = {
    "url": "http://ollama:11434",
    "model": "llama2:7b",
    "temperature": 0.7,
    "max_tokens": 500
}

# Configuration pour Fanvue
fanvue_config = {
    "api_key": "your_api_key",
    "auto_response": True,
    "sales_enabled": True,
    "ollama_config": ollama_config
}

manager.configure_platform("fanvue", fanvue_config)
```

### Gestion d'une conversation avec Ollama
```python
# Gestion automatique d'une conversation
conversation = manager.handle_message(
    platform="fanvue",
    user_id="user_123",
    message="Salut, tu as des photos de pieds ?",
    context={
        "subscription_level": "premium",
        "purchase_history": ["pack_001"],
        "ollama_model": "mistral:7b"
    }
)

# RÃ©ponse automatique gÃ©nÃ©rÃ©e par Ollama
response = conversation.get_response()
# "Salut beautÃ© ğŸ˜˜ Oui j'ai un pack pieds exclusif avec 10 photos hyper sexy pour seulement 10â‚¬ !"
```

---

## ğŸ”§ Configuration Docker

### Dockerfile pour le module chatbot
```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Installation des dÃ©pendances
COPY requirements.txt .
RUN pip install -r requirements.txt

# Installation d'Ollama (si nÃ©cessaire)
RUN curl -fsSL https://ollama.ai/install.sh | sh

# Copie du code
COPY . .

# Exposition du port
EXPOSE 8000

# Commande de dÃ©marrage
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Requirements pour Ollama
```txt
fastapi==0.104.1
uvicorn==0.24.0
requests==2.31.0
pydantic==2.5.0
python-dotenv==1.0.0
redis==5.0.1
psycopg2-binary==2.9.9
langchain==0.1.0
langchain-community==0.0.10
chromadb==0.4.18
sentence-transformers==2.2.2
```

---

*Document crÃ©Ã© le : [Date]*
*Version : 2.0*
*DerniÃ¨re mise Ã  jour : [Date]*
*Approche : Open Source avec Ollama pour LLM local*
